{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import language_check\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots (url):\n",
    "    response = requests.get(url+\"/robots.txt\")\n",
    "    robots = response.text\n",
    "    return robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COPY FROM ANDREA'S SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(url, find, headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # make sure that the page exist\n",
    "    if r.status_code == 200:\n",
    "        html = r.content\n",
    "        try:\n",
    "            dammit = UnicodeDammit(html)\n",
    "            #print(dammit.unicode_markup)\n",
    "            encoding = dammit.original_encoding\n",
    "            soup = BeautifulSoup(html, 'lxml', exclude_encodings=[encoding])\n",
    "            new_encoding = soup.original_encoding\n",
    "            #print(soup.original_encoding)\n",
    "            soup = BeautifulSoup(html, 'lxml',  exclude_encodings=[new_encoding])\n",
    "            #print(soup.original_encoding)\n",
    "            title = soup.find('h1')\n",
    "            if title is not None:\n",
    "                title_text = title.text.strip()\n",
    "            #article = soup.find(find)   \n",
    "            article = soup.find_all(find)\n",
    "            new_article = []\n",
    "            for i in article:\n",
    "                dammit = UnicodeDammit(i.text.strip().encode(), smart_quotes_to=\"ascii\").unicode_markup\n",
    "                new_article.append((dammit.encode('ascii', 'ignore')).decode(\"utf-8\"))\n",
    "            sentences = []\n",
    "            for i in new_article:\n",
    "                sentences.append(sent_tokenize(i))\n",
    "            sentences = sum(sentences, [])\n",
    "            return sentences\n",
    "        except TypeError:\n",
    "            print(\"Type error with url: \", url)\n",
    "\n",
    "def multi_sites(url_list, find):\n",
    "    all_sents = []\n",
    "    new_url = []\n",
    "    for i in url_list:\n",
    "        this_url_sents = get_sentences(i, find)\n",
    "        if not isinstance(this_url_sents, type(None)):\n",
    "            new_url.append(i)\n",
    "            all_sents.append(this_url_sents)\n",
    "    all_sents = sum(all_sents,[])\n",
    "    return all_sents, new_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_container(url, find, headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # make sure that the page exist\n",
    "    if r.status_code == 200:\n",
    "        html = r.content\n",
    "        try:\n",
    "            dammit = UnicodeDammit(html)\n",
    "            #print(dammit.unicode_markup)\n",
    "            encoding = dammit.original_encoding\n",
    "            soup = BeautifulSoup(html, 'lxml', exclude_encodings=[encoding])\n",
    "            new_encoding = soup.original_encoding\n",
    "            #print(soup.original_encoding)\n",
    "            soup = BeautifulSoup(html, 'lxml',  exclude_encodings=[new_encoding])\n",
    "            #print(soup.original_encoding)\n",
    "            title = soup.find('h1')\n",
    "            if title is not None:\n",
    "                title_text = title.text.strip()\n",
    "            #article = soup.find(find)   \n",
    "            article = soup.find_all(find).text\n",
    "            new_article = []\n",
    "            for i in article:\n",
    "                dammit = UnicodeDammit(i.text.strip().encode(), smart_quotes_to=\"ascii\").unicode_markup\n",
    "                new_article.append((dammit.encode('ascii', 'ignore')).decode(\"utf-8\"))\n",
    "            sentences = []\n",
    "            for i in new_article:\n",
    "                sentences.append(sent_tokenize(i))\n",
    "            sentences = sum(sentences, [])\n",
    "            return sentences\n",
    "        except TypeError:\n",
    "            print(\"Type error with url: \", url)\n",
    "    \n",
    "def multi_sites_container(url_list, find):\n",
    "    all_sents = []\n",
    "    new_url = []\n",
    "    for i in url_list:\n",
    "        this_url_sents = get_sentences_container(i, find)\n",
    "        if not isinstance(this_url_sents, type(None)):\n",
    "            new_url.append(i)\n",
    "            all_sents.append(this_url_sents)\n",
    "    all_sents = sum(all_sents,[])\n",
    "    return all_sents, new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(batch, file_name):\n",
    "    batch = np.asarray(batch)\n",
    "    pd_df = pd.DataFrame(batch)\n",
    "    pd_df.to_csv(file_name, index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_check(batch):\n",
    "    grammar_tool = language_check.LanguageTool('en-GB')\n",
    "    checked = []\n",
    "    for i in batch:\n",
    "        corrected = ''\n",
    "        matches = grammar_tool.check(i)\n",
    "        checked.append(language_check.correct(i, matches))\n",
    "    return checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_url = 'http://www.dating-relationship-advice-for-women.com'\n",
    "robots = get_robots(first_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "User-agent: *\n",
      "Disallow: /cgi-bin\n",
      "Disallow: /wp-admin\n",
      "Disallow: /wp-includes\n",
      "Disallow: /wp-content/plugins/\n",
      "Disallow: /wp-content/cache/\n",
      "Disallow: /wp-content/themes/\n",
      "Disallow: */trackback/\n",
      "Disallow: */feed/\n",
      "Disallow: /*/feed/rss/$\n",
      "Disallow: /category/*\n"
     ]
    }
   ],
   "source": [
    "print(robots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm guessing these are html folders.Trying to see how to verify in the articles url that anything is in those folders. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
